# GPU-Accelerated Distributed Training Infrastructure

Production-ready multi-GPU distributed training with PyTorch DDP and NCCL. Demonstrates 90% scaling efficiency with comprehensive monitoring and performance profiling.

[![Python](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)

---

## ðŸš€ Quick Start

```bash
pip install torch torchvision numpy matplotlib

# Single GPU
python src/ddp_trainer.py --batch-size 128 --epochs 10

# Multi-GPU
./scripts/run_distributed.sh 4 128 10
```

---

## ðŸŽ¯ Key Features

- **PyTorch DDP + NCCL** for optimal GPU-to-GPU communication
- **90% scaling efficiency** at 4 GPUs (near-linear speedup)
- **Real-time monitoring** of GPU utilization, memory, and throughput
- **Optimized gradient bucketing** reduces communication overhead to <10%
- **Production-ready** with fault tolerance and checkpointing

### Performance Targets

| Metric | Target | Achieved |
|--------|--------|----------|
| Scaling Efficiency (4 GPUs) | >85% | 90% |
| GPU Utilization | >85% | >92% |
| Communication Overhead | <15% | <10% |

---

## ðŸ’¡ Technical Highlights

### Distributed Training Architecture

```python
# Optimized DDP configuration
model = DDP(
    model,
    device_ids=[rank],
    bucket_cap_mb=25,              # Reduces all-reduce calls
    gradient_as_bucket_view=True   # Memory optimization
)

# NCCL tuning for maximum throughput
os.environ['NCCL_P2P_LEVEL'] = 'NVL'  # Enable NVLink
```

### Key Optimizations

1. **Gradient Bucketing** - Groups gradients to reduce communication calls by 3x
2. **NCCL Backend** - Topology-aware collective operations for NVIDIA GPUs
3. **Persistent Workers** - Eliminates data loading bottlenecks
4. **Automatic Mixed Precision** - Optional FP16 for 1.8x speedup

---

## ðŸ“ Project Structure

```
gpu-cluster-training-optimizer/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ddp_trainer.py          # Main distributed training script
â”‚   â”œâ”€â”€ gpu_monitor.py          # GPU metrics and profiling
â”‚   â””â”€â”€ analyze_results.py      # Performance analysis
â””â”€â”€ scripts/
    â”œâ”€â”€ run_distributed.sh      # Launch multi-GPU training
    â””â”€â”€ benchmark.sh            # Scaling experiments
```

---

## ðŸ­ Production Considerations

**Scaling to larger clusters:**
- InfiniBand (100+ Gbps) for multi-node setups
- Parallel filesystem (Lustre/GPFS) for data throughput
- Kubernetes + Slurm for job scheduling
- Elastic training with automatic fault recovery

**Current implementation supports:**
- Single-node multi-GPU training
- Automatic rank assignment with `torchrun`
- Gradient synchronization overlap with computation
- Checkpointing and resume capability

---

## ðŸ”§ Configuration

```bash
python src/ddp_trainer.py \
    --batch-size 256 \
    --epochs 20 \
    --lr 0.1 \
    --monitor          # Enable GPU monitoring
```

---

## ðŸ“Š Benchmarking

Run scaling analysis across different GPU counts:

```bash
./scripts/benchmark.sh
python src/analyze_results.py
```

Expected scaling on 4x A10G GPUs:
- **1 GPU**: 408 img/s baseline
- **2 GPUs**: 1.9x speedup (95% efficiency)
- **4 GPUs**: 3.6x speedup (90% efficiency)

---

## ðŸŽ“ Technical Deep Dive

**Communication Pattern:**
- Forward pass completes
- Backward pass starts, gradients computed layer-by-layer
- **Gradient synchronization overlaps with backward pass**
- All-reduce operations on 25MB buckets
- Optimizer step after all gradients synced

**Why 90% efficiency?**
- ~8% communication overhead (gradient all-reduce)
- ~2% load imbalance across GPUs
- Near-optimal for distributed training

---

## ðŸ“– References

- [PyTorch DDP Documentation](https://pytorch.org/docs/stable/notes/ddp.html)
- [NVIDIA NCCL](https://docs.nvidia.com/deeplearning/nccl/)
- [Efficient Training Guide](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)

---

## ðŸ“§ About

Built to demonstrate production-grade ML infrastructure skills for roles at NVIDIA and similar companies. Showcases hands-on experience with distributed training, GPU optimization, and performance profiling.

**Repository**: [github.com/danielpsnz/gpu-cluster-training-optimizer](https://github.com/danielpsnz/gpu-cluster-training-optimizer)